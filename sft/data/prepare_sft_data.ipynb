{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ec7ca5-a9e3-4d79-83be-d1e0a57745a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from xml.etree import ElementTree as ET\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e3f90-ffa8-4dcf-94df-cdbbb17065b6",
   "metadata": {},
   "source": [
    "## get gsm8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95cab01a-33ee-41cf-953a-0e30cec99e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "data = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c00ec1c-a682-4bdd-824d-006360c856e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in data.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    solution = row[\"answer\"]\n",
    "    \n",
    "    cot, answer = solution.split(\"#### \")\n",
    "\n",
    "    data.loc[idx, 'cot']    = cot\n",
    "    data.loc[idx, 'answer'] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a58bd681-c387-4623-a7b3-78fb328dae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('gsm8k_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6123c-1c8c-449c-8771-07c8cc313025",
   "metadata": {},
   "source": [
    "## parse the gpt-4o evaluator data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ac29c-e632-4782-8f17-508b2aecc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/scratch/hd2584/llm_marl/sft/data/gpt-4o/gsm8k_train_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f5cb476-11c0-4c70-8a16-3315053204af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags(text, agent='generator'):\n",
    "    \"\"\"\n",
    "    Parse XML‑like tags from text using regex. Returns a dict:\n",
    "      - for agent='generator': keys \"think\" and \"answer\"\n",
    "      - for agent='evaluator': keys \"evaluate\" and \"verify\"\n",
    "    Each value is a list of the inner texts of those tags.\n",
    "    \"\"\"\n",
    "    # choose which tags to look for\n",
    "    if agent == 'generator':\n",
    "        tag_pairs = [(\"think\", \"think\"), (\"answer\", \"answer\")]\n",
    "    else:  # evaluator\n",
    "        tag_pairs = [(\"evaluate\", \"evaluate\"), (\"verify\", \"verify\")]\n",
    "\n",
    "    out = {}\n",
    "    for key, tag in tag_pairs:\n",
    "        # non‑greedy match between <tag>...</tag>, including newlines\n",
    "        pattern = rf\"<{tag}>(.*?)</{tag}>\"\n",
    "        matches = re.findall(pattern, text, flags=re.DOTALL)\n",
    "        # strip leading/trailing whitespace from each match\n",
    "        out[key] = [m.strip() for m in matches]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "369f0a2c-e9d0-48eb-b724-d1b7801a1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in data.iterrows():\n",
    "    evaluate = row['evaluate']\n",
    "    if 'evaluate>' not in evaluate:\n",
    "        evaluate = evaluate.replace(\"<verify>\", \"</evaluate> <verify>\")\n",
    "    evaluate = evaluate.replace(\"<answer>\", \"<verify>\").replace(\"</answer>\", \"</verify>\")   \n",
    "    evaluate = evaluate.replace(\"< /evaluate>\", \"</evaluate>\").replace(\"< /verify>\", \"</verify>\")\n",
    "    evaluate = evaluate.replace(\"<<\", \"\").replace(\">>\", \"\")\n",
    "    data.loc[idx, 'evaluate'] = evaluate # prepare the training data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbd554d-91ed-4c22-83b2-003ab1685b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Accuracy: 0.7274\n",
      "- Generator correct, evaluator correct: 4707 0.6299\n",
      "- Generator correct, evaluator wrong: 728 0.0974\n",
      "- Generator wrong, evaluator correct: 1135 0.1519\n",
      "- Generator wrong, evaluator wrong: 896 0.1199\n",
      "- No verification: 0 0.0000\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "correct_cnt = 0\n",
    "gen_c_ver_c, gen_c_ver_w, gen_w_ver_c, gen_w_ver_w, no_ver = 0, 0, 0, 0, 0\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    response = row['response']\n",
    "    evaluate = row['evaluate']\n",
    "    answer = row['answer']\n",
    "\n",
    "    try:\n",
    "        tags_gen = extract_tags(\"<think>\" + response.replace(\"<<\", \"\").replace(\">>\", \"\"))\n",
    "    except ET.ParseError:\n",
    "        tags_gen = {\"think\": [], \"answer\": []}\n",
    "        \n",
    "    try:\n",
    "        tags_eval = extract_tags(\"<evaluate>\" + evaluate, agent = 'evaluator')\n",
    "    except ET.ParseError:\n",
    "        tags_eval = {\"evaluate\": [], \"verify\": []}\n",
    "\n",
    "\n",
    "    if any((answer in attempt.replace('$', '').replace('ml', '').replace('.00', '').split()) for attempt in tags_gen[\"answer\"]):\n",
    "        data.loc[idx, 'res_correct'] = 1\n",
    "        gen_is_correct = True\n",
    "        correct_cnt += 1\n",
    "    else:\n",
    "        data.loc[idx, 'res_correct'] = 0\n",
    "        gen_is_correct = False\n",
    "\n",
    "    if len(tags_eval[\"verify\"])==0:\n",
    "        # print(f'{idx}---')\n",
    "        # print(evaluate)\n",
    "        no_ver += 1\n",
    "    else:\n",
    "        verification = tags_eval[\"verify\"][0].lower()\n",
    "        if gen_is_correct and verification == 'correct':\n",
    "            # print('Generator correct, evaluator correct')\n",
    "            data.loc[idx, 'eval_correct'] = 1\n",
    "            gen_c_ver_c += 1\n",
    "        elif gen_is_correct and verification == 'wrong' :\n",
    "            # print('Generator correct, evaluator wrong')\n",
    "            # print(idx)\n",
    "            gen_c_ver_w += 1\n",
    "            data.loc[idx, 'eval_correct'] = 0\n",
    "        elif (not gen_is_correct) and verification == 'wrong':\n",
    "            # print('Generator wrong, evaluator correct')\n",
    "            gen_w_ver_c += 1\n",
    "            data.loc[idx, 'eval_correct'] = 1\n",
    "        elif (not gen_is_correct) and verification == 'correct':\n",
    "            # print('Generator wrong, evaluator wrong')\n",
    "            # print(idx)\n",
    "            gen_w_ver_w += 1\n",
    "            data.loc[idx, 'eval_correct'] = 0\n",
    "        \n",
    "        \n",
    "print('- Accuracy:', f'{correct_cnt/len(data):.4f}')\n",
    "print('- Generator correct, evaluator correct:', gen_c_ver_c, f'{gen_c_ver_c/len(data):.4f}')\n",
    "print('- Generator correct, evaluator wrong:', gen_c_ver_w, f'{gen_c_ver_w/len(data):.4f}')\n",
    "print('- Generator wrong, evaluator correct:', gen_w_ver_c, f'{gen_w_ver_c/len(data):.4f}')\n",
    "print('- Generator wrong, evaluator wrong:', gen_w_ver_w, f'{gen_w_ver_w/len(data):.4f}')\n",
    "print('- No verification:', no_ver, f'{no_ver/len(data):.4f}')\n",
    "# len(data[data['res_correct']==1][data['eval_correct']==0])/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b718c649-bd16-4ae9-9213-f007b36479d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7473"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952dcb6-744c-4c9d-b6dd-afee2ba82f73",
   "metadata": {},
   "source": [
    "## get evaluator sft dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0daf6bf-2369-4936-b149-229d598de253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5842"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data[data['eval_correct']==1]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbc5350c-89f9-4cee-b8bd-9955f3c44dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.8, random_state=42)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a3ce39-50b4-4226-9985-ea9728d4908f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res_correct\n",
       "1.0    3769\n",
       "0.0     905\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.res_correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32098545-9051-499c-8622-bd5da3637254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a7d3cd78-e9a2-4fe3-828a-8a6f8922632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('eval_sft_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b018bb2-fa2d-4ce7-a135-65a87b7e3f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res_correct\n",
       "1.0    938\n",
       "0.0    230\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.res_correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "605b4c1b-be1d-40e9-8021-4aade4b76f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2871f0dc-0a1f-4dbe-b703-3f5f28659aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('eval_sft_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d07ebab-c0b0-4aba-ab2f-97201ff89a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./eval_sft_data/eval_sft_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23305307-65fc-4c00-8fff-e545ea7334d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. \"\n",
    "    \"The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \"\n",
    "    \"The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, \"\n",
    "    \"i.e., <think>reasoning process here</think> <answer>answer here</answer>. User: %s. Assistant: <think>\"\n",
    ")\n",
    "\n",
    "\n",
    "EVALUATION_PROMPT = (\n",
    "    \"%s. \"\n",
    "    \"The Assistant evaluates the problem and the solution. The assistant first evaluates and finds out where the solution is wrong. \"\n",
    "    \"Then the assistant provides the verification. \"\n",
    "    \"The evaluation process and verification are enclosed within <evaluate></evaluate> and <verify></verify> tags, respectively, \"\n",
    "    \"i.e., <evaluate>evaluation process here</evaluate> <verify>verification here</verify>. Verification is limited to 'correct' or 'wrong'. Assistant:<evaluate>\"\n",
    ")\n",
    "\n",
    "data['gen_prompt'] = [BASE_PROMPT % question for question in data['question']]\n",
    "for idx, row in data.iterrows():\n",
    "    res = row['gen_prompt'] + row['response']\n",
    "    data.loc[idx, 'eval_prompt'] = EVALUATION_PROMPT % res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68189207-f9fa-468e-a511-481fc223ca77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4674"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae31b4c-af34-4645-86e2-c94f7d91278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_dict(orient=\"records\")\n",
    "path = f\"/scratch/hd2584/llm_marl/sft/data/eval_sft_data/eval_sft_train.json\"\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22785c-65a4-4de3-81c8-53be1de77844",
   "metadata": {},
   "source": [
    "## get generator sft dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21506d49-5e82-4f64-a077-e07dac18a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./gsm8k/gsm8k_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee4d1539-5683-4bc0-9494-b6d79a9dba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. \"\n",
    "    \"The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \"\n",
    "    \"The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, \"\n",
    "    \"i.e., <think>reasoning process here</think> <answer>answer here</answer>. User: %s. Assistant: <think>\"\n",
    ")\n",
    "\n",
    "data['prompt'] = [BASE_PROMPT % question for question in data['question']]\n",
    "data['target'] = [f\"{row['cot']}</think> <answer>{row['answer']}</answer>\" for id, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adeec18-71b3-4ad8-b42b-56413637575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_dict(orient=\"records\")\n",
    "path = f\"/scratch/hd2584/llm_marl/sft/data/gen_sft_data/gen_sft_train.json\"\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4829f1-158e-458d-b60b-1e6ab2c3e4c4",
   "metadata": {},
   "source": [
    "## get mixed sft dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41b9c2fe-3642-4b5d-b83f-406189b26065",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./eval_sft_data/eval_sft_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "424f8f59-a011-4b68-ade0-235c44c65fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. \"\n",
    "    \"The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \"\n",
    "    \"The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, \"\n",
    "    \"i.e., <think>reasoning process here</think> <answer>answer here</answer>. User: %s. Assistant: <think>\"\n",
    ")\n",
    "\n",
    "\n",
    "EVALUATION_PROMPT = (\n",
    "    \"%s. \"\n",
    "    \"The Assistant evaluates the problem and the solution. The assistant first evaluates and finds out where the solution is wrong. \"\n",
    "    \"Then the assistant provides the verification. \"\n",
    "    \"The evaluation process and verification are enclosed within <evaluate></evaluate> and <verify></verify> tags, respectively, \"\n",
    "    \"i.e., <evaluate>evaluation process here</evaluate> <verify>verification here</verify>. Verification is limited to 'correct' or 'wrong'. Assistant:<evaluate>\"\n",
    ")\n",
    "\n",
    "data['gen_prompt'] = [BASE_PROMPT % question for question in data['question']]\n",
    "for idx, row in data.iterrows():\n",
    "    res = row['gen_prompt'] + row['response']\n",
    "    data.loc[idx, 'eval_prompt'] = EVALUATION_PROMPT % res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6759ccf-86a2-49ae-9c2c-31b6c0646c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrong = data[data['res_correct']==0]\n",
    "n = len(data_wrong)\n",
    "data_correct = data[data['res_correct']==1].sample(n, random_state=42)\n",
    "data_final = pd.concat([data_correct, data_wrong])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12c61b35-fec1-4c2f-87d6-8b8d1a87642b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4efa6ac-b4aa-4fd7-a4bf-afe4b1b8752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_final.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0504d748-5b7a-41d0-8d56-6e943e50dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_a = data_final.copy()\n",
    "data_final_a['prompt'] = data_final_a['gen_prompt']\n",
    "data_final_a['target'] = [f\"{row['cot']}</think> <answer>{row['answer']}</answer>\" for id, row in data_final_a.iterrows()]\n",
    "data_final_b = data_final.copy()\n",
    "data_final_b['prompt'] = data_final_a['eval_prompt']\n",
    "data_final_b['target'] = data_final_a['evaluate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "428daa38-ffb0-4764-bf9a-1d7c7c0de091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3620"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final = pd.concat([data_final_a, data_final_b])\n",
    "data_final.reset_index(drop=True, inplace=True)\n",
    "len(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dbeb7-3fc3-4e18-b644-3039d353e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_final.to_dict(orient=\"records\")\n",
    "path = f\"/scratch/hd2584/llm_marl/sft/data/eval_sft_data/mix_sft_train.json\"\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_final, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b1c96-a309-4a06-a8b2-bec9a8d106a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = pd.read_json(\"/scratch/hd2584/llm_marl/sft/data/eval_sft_data/mix_sft_train.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
