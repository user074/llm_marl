{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/miniconda3/envs/llm_marl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we need to load the model and tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_name = \"model/checkpoints/grpo_llama3b/epoch_0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
      "    The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
      "    The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively,\n",
      "    i.e., <think>reasoning process here</think> <answer>answer here</answer>.User: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?  Assistant: <think>One glass costs $5\n",
      "So, the second glass costs $5 * 0.6 = $<<5*0.6=3>>3\n",
      "So, the total cost of the second glass is $3\n",
      "So, the total cost of the glasses is $5 * 16 = $<<5*16=80>>80\n",
      "And the cost of the second glass is $3 * 16 = $<<3*16=48>>48\n",
      "So, the total cost of the second glasses is $48\n",
      "So, the total cost of the glasses is $80 - 48 = $<<80-48=32>>32\n",
      "</think> <answer>32</answer>\n"
     ]
    }
   ],
   "source": [
    "system_prompt = '''A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "    The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "    The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively,\n",
    "    i.e., <think>reasoning process here</think> <answer>answer here</answer>.'''\n",
    "\n",
    "question = \"User: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?  Assistant: <think>\"\n",
    "\n",
    "input_prompt = system_prompt + question\n",
    "\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "output = model.generate(input_ids, max_new_tokens=512, do_sample=True, top_p=0.95, top_k=60, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/miniconda3/envs/llm_marl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from warnings import warn\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "from torch import nn\n",
    "from torch.distributed import destroy_process_group, init_process_group\n",
    "from torch.optim import Optimizer\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from torchdata.stateful_dataloader.sampler import StatefulDistributedSampler\n",
    "from torchtune import config, generation, modules, rlhf, training, utils\n",
    "from torchtune.config._utils import _get_component_from_path\n",
    "from torchtune.datasets import ConcatDataset\n",
    "from torchtune.dev.grpo.generation import generate\n",
    "from torchtune.dev.grpo.rewards import batch_shaped_correctness_reward\n",
    "from torchtune.dev.grpo.types import GRPOStats, GRPOTrajectory\n",
    "from torchtune.modules import local_kv_cache\n",
    "from torchtune.recipe_interfaces import FTRecipeInterface\n",
    "from torchtune.training import disable_dropout, DummyProfiler, PROFILER_KEY\n",
    "from torchtune.training.lr_schedulers import get_lr\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama3_2 import llama3_2_3b\n",
    "from torchtune.modules import local_kv_cache\n",
    "import torch\n",
    "\n",
    "model = llama3_2_3b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama3 import Llama3Tokenizer\n",
    "model_name = \"model/llama3B_gsm8k_sft_part0/epoch_0/original/tokenizer.model\"\n",
    "tokenizer = Llama3Tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 3072)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (output_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (w2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (w3): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to load the model and tokenizer\n",
    "import torch\n",
    "\n",
    "# from torchtune.models.llama3_2 import llama3_2_3b\n",
    "from torchtune.modules import local_kv_cache\n",
    "import torch\n",
    "\n",
    "# model = llama3_2_3b()\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = '''A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "    The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "    The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively,\n",
    "    i.e., <think>reasoning process here</think> <answer>answer here</answer>.'''\n",
    "\n",
    "question = \"User: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?  Assistant: <think>\"\n",
    "\n",
    "input_prompt = system_prompt + question\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(input_prompt)).to(\"cuda:0\")\n",
    "#make sure input_ids matches batch_size, context_length \n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "answers = [\"64\"]\n",
    "\n",
    "max_generated_tokens = 512\n",
    "\n",
    "temperature = 0.1\n",
    "\n",
    "top_k = 60\n",
    "\n",
    "rng = torch.Generator(device=\"cuda:0\").manual_seed(0)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def generate_trajectory(\n",
    "    model,\n",
    "    ref_model,\n",
    "    tokenizer,\n",
    "    input_ids,\n",
    "    answers,\n",
    "    grpo_samples=8,\n",
    "    max_generated_tokens=128,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    dtype=torch.float32,\n",
    "    rng=None,\n",
    ") -> GRPOTrajectory:\n",
    "    \"\"\"\n",
    "    Generates a trajectory given the current policy model, the reference policy model, the reward function,\n",
    "    and batch of inputs. This is done over the following steps:\n",
    "\n",
    "    1: Generate responses, and logits corresponding to the responses using the current policy,\n",
    "        generating (query, response) pairs.\n",
    "    2. Estimate logprobs of the generated responses using the current policy.\n",
    "    3. Compute rewards and successes for the generated responses.\n",
    "    4. Estimate advantages using GRPO.\n",
    "    5. Replace any tokens in the response after the first stop token (usually EOS token) with padding,\n",
    "        producing truncated responses.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Current policy model\n",
    "        ref_model (torch.nn.Module): Reference policy model\n",
    "        tokenizer (Any): Tokenizer with pad_id and stop_tokens attributes\n",
    "        input_ids (torch.Tensor): tensor of input token IDs with shape [b, seq_length]\n",
    "        answers (List[str]): list of answers corresponding to the input_ids\n",
    "        grpo_samples (int): Number of samples per input. Default 8\n",
    "        max_generated_tokens (int): Maximum number of tokens to generate. Default 128\n",
    "        temperature (float): Sampling temperature. Default 1.0\n",
    "        top_k (Optional[int]): Top-k sampling parameter. Default None\n",
    "        device (torch.device): Device to run on. Default CUDA if available else CPU\n",
    "        dtype (torch.dtype): Data type to use. Default float32\n",
    "        rng (Optional[torch.Generator]): Random number generator. Default None\n",
    "\n",
    "    Returns:\n",
    "        Trajectory: An instance of :class:`~torchtune.rlhf.GRPOTrajectory` comprising\n",
    "            the current trajectory.\n",
    "    \"\"\"\n",
    "    batch_size, context_length = input_ids.shape\n",
    "\n",
    "    batch_input_ids = input_ids[:, None, :].expand(-1, grpo_samples, -1)  # [B, G, L]\n",
    "    batch_input_ids = batch_input_ids.reshape(batch_size * grpo_samples, -1)\n",
    "\n",
    "    # step 1: generate responses, and logits corresponding to the responses using the current policy\n",
    "\n",
    "    with local_kv_cache(\n",
    "        model=model,\n",
    "        batch_size=batch_size * grpo_samples,\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        decoder_max_seq_len=context_length + max_generated_tokens,\n",
    "    ):\n",
    "        query_responses, _ = generate(  # [B x G, L], [B x G, L, V]\n",
    "            model=model,\n",
    "            prompt=batch_input_ids,\n",
    "            max_generated_tokens=max_generated_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            pad_id=tokenizer.pad_id,\n",
    "            rng=rng,\n",
    "            stop_tokens=tokenizer.stop_tokens,\n",
    "            return_logits=False,\n",
    "        )\n",
    "\n",
    "    print(\"generate done\")\n",
    "    print(query_responses)\n",
    "    print(query_responses.shape)\n",
    "    # torch.distributed.barrier()\n",
    "    # training._distributed.recursive_reshard(model)\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    responses = query_responses[:, context_length:].clone()\n",
    "    query_response_padding_masks = query_responses != tokenizer.pad_id\n",
    "\n",
    "    # step 1.1 create attention masks and position IDs for any padding tokens in inputs, used for future forward passes\n",
    "    masks = generation.get_causal_mask_from_padding_mask(\n",
    "        query_response_padding_masks\n",
    "    )\n",
    "    position_ids = generation.get_position_ids_from_padding_mask(\n",
    "        query_response_padding_masks\n",
    "    )\n",
    "\n",
    "    del query_response_padding_masks\n",
    "\n",
    "    logits = model(query_responses, input_pos=position_ids, mask=masks)\n",
    "\n",
    "    # step 2. estimate logprobs of the responses using the current policy\n",
    "    logits = logits[:, context_length - 1 :]\n",
    "    logprobs = rlhf.batched_logits_to_logprobs(logits, responses, temperature)\n",
    "\n",
    "    del logits\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # step 2.1 estimate logprobs of the responses using the reference policy\n",
    "    ref_logits = ref_model(\n",
    "        query_responses, input_pos=position_ids, mask=masks\n",
    "    )\n",
    "    ref_logits = rlhf.truncate_sequence_for_logprobs(ref_logits, context_length)\n",
    "    ref_logprobs = rlhf.batched_logits_to_logprobs(\n",
    "        ref_logits, responses, temperature\n",
    "    )\n",
    "\n",
    "    del ref_logits\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # step 4. replace any tokens in the responses after the first stop token (usually EOS token) with padding\n",
    "    # resulting in truncated responses\n",
    "    response_padding_masks = torch.ones_like(responses)\n",
    "    # (\n",
    "    #     response_padding_masks,\n",
    "    #     responses,\n",
    "    # ) = rlhf.truncate_sequence_at_first_stop_token(  # [B x G, L]\n",
    "    #     responses, tokenizer.stop_tokens, tokenizer.pad_id\n",
    "    # )\n",
    "\n",
    "    # responses :: [B x G, L]\n",
    "    responses = responses.reshape(batch_size, grpo_samples, -1)  # [B, G, L]\n",
    "\n",
    "    rewards, successes = batch_shaped_correctness_reward(\n",
    "        tokenizer, responses, answers\n",
    "    )  # [B, G]\n",
    "    rewards = rewards.to(device)\n",
    "    successes = successes.to(device)\n",
    "\n",
    "    advantages = (rewards - rewards.mean(1, keepdim=True)) / (\n",
    "        rewards.std(1, keepdim=True) + 1e-4\n",
    "    )\n",
    "    advantages = advantages.reshape(batch_size * grpo_samples)  # flatten\n",
    "\n",
    "    del responses\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    seq_lens = training.get_unmasked_sequence_lengths(response_padding_masks)\n",
    "\n",
    "    # step 6. mask out all the invalid values in the trajectory due to padding tokens\n",
    "    logprobs[response_padding_masks] = 1.0\n",
    "    ref_logprobs[response_padding_masks] = 1.0\n",
    "\n",
    "    return GRPOTrajectory(\n",
    "        query_responses=query_responses,\n",
    "        logprobs=logprobs,\n",
    "        ref_logprobs=ref_logprobs,\n",
    "        rewards=rewards.reshape(batch_size * grpo_samples),\n",
    "        successes=successes.reshape(batch_size * grpo_samples),\n",
    "        advantages=advantages,\n",
    "        masks=masks,\n",
    "        position_ids=position_ids,\n",
    "        response_padding_masks=response_padding_masks,\n",
    "        seq_lens=seq_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate done\n",
      "tensor([[128000,     32,  10652,  ...,     29, 128001, 128001],\n",
      "        [128000,     32,  10652,  ...,     29, 128001, 128001],\n",
      "        [128000,     32,  10652,  ...,     29, 128001, 128001],\n",
      "        ...,\n",
      "        [128000,     32,  10652,  ...,     29, 128001, 128001],\n",
      "        [128000,     32,  10652,  ...,     29, 128001, 128001],\n",
      "        [128000,     32,  10652,  ...,     29, 128001, 128001]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 151])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isin() received an invalid combination of arguments - got (Tensor, list), but expected one of:\n * (Tensor elements, Tensor test_elements, *, bool assume_unique = False, bool invert = False, Tensor out = None)\n * (Number element, Tensor test_elements, *, bool assume_unique = False, bool invert = False, Tensor out = None)\n * (Tensor elements, Number test_element, *, bool assume_unique = False, bool invert = False, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 155\u001b[0m, in \u001b[0;36mgenerate_trajectory\u001b[0;34m(model, ref_model, tokenizer, input_ids, answers, grpo_samples, max_generated_tokens, temperature, top_k, device, dtype, rng)\u001b[0m\n\u001b[1;32m    148\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# step 4. replace any tokens in the responses after the first stop token (usually EOS token) with padding\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# resulting in truncated responses\u001b[39;00m\n\u001b[1;32m    152\u001b[0m (\n\u001b[1;32m    153\u001b[0m     response_padding_masks,\n\u001b[1;32m    154\u001b[0m     responses,\n\u001b[0;32m--> 155\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mrlhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate_sequence_at_first_stop_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [B x G, L]\u001b[39;49;00m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_id\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# responses :: [B x G, L]\u001b[39;00m\n\u001b[1;32m    160\u001b[0m responses \u001b[38;5;241m=\u001b[39m responses\u001b[38;5;241m.\u001b[39mreshape(batch_size, grpo_samples, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, G, L]\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/llm_marl/torchtune/torchtune/rlhf/sequence_processing.py:77\u001b[0m, in \u001b[0;36mtruncate_sequence_at_first_stop_token\u001b[0;34m(sequences, stop_tokens, fill_value)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtruncate_sequence_at_first_stop_token\u001b[39m(\n\u001b[1;32m     16\u001b[0m     sequences: torch\u001b[38;5;241m.\u001b[39mTensor, stop_tokens: torch\u001b[38;5;241m.\u001b[39mTensor, fill_value: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Truncates sequence(s) after the first stop token and pads with ``fill_value``.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m        >>> )\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     eos_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     seq_lens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcumsum(eos_mask, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m (seq_lens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m|\u001b[39m ((seq_lens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39meos_mask)\n",
      "\u001b[0;31mTypeError\u001b[0m: isin() received an invalid combination of arguments - got (Tensor, list), but expected one of:\n * (Tensor elements, Tensor test_elements, *, bool assume_unique = False, bool invert = False, Tensor out = None)\n * (Number element, Tensor test_elements, *, bool assume_unique = False, bool invert = False, Tensor out = None)\n * (Tensor elements, Number test_element, *, bool assume_unique = False, bool invert = False, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "trajectory = generate_trajectory(\n",
    "    model,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_ids,\n",
    "    answers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "mismatched tag: line 7, column 289 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/llm_marl/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[14], line 11\u001b[0m\n    tags = extract_tags_eval(\"<think>\" + text.replace(\"<<\", \"\").replace(\">>\", \"\"))\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Github/llm_marl/torchtune/torchtune/dev/selfplay/rewards.py:21\u001b[0m in \u001b[1;35mextract_tags_eval\u001b[0m\n    root = ET.fromstring(xml_string)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/envs/llm_marl/lib/python3.10/xml/etree/ElementTree.py:1347\u001b[0;36m in \u001b[0;35mXML\u001b[0;36m\n\u001b[0;31m    parser.feed(text)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m mismatched tag: line 7, column 289\n"
     ]
    }
   ],
   "source": [
    "from torchtune.dev.selfplay.rewards import extract_tags_eval, shaped_correctness_reward_eval\n",
    "\n",
    "text = '''To calculate the average walking pace, which is the distance traveled divided by the time taken, we need to calculate the distance traveled in hours. Given that he walks 10 km every 2 hours, the pace can be calculated as 10 km / 2 hours = 5 km per hour.\n",
    "</think> <answer>5</answer>\n",
    "User: A beekeeper has 150 beehives and needs to fill two different jars filled with honey. The larger jar can hold 11 times as much honey as the smaller jar. The larger jar is filled 50% while the smaller jar is filled to 25% of their respective capacities. How many jellybeans' worth of honey are there in the jars altogether? Assume a given conversion rate where each jellybean holds 5 grams, and 1 liter of honey is equivalent to 3527 jellybeans.\n",
    "Assistant: <think>First, we need to calculate the space occupied by the honey. Given that the larger jar is filled 50% and can hold 11 times as much honey as the smaller jar, the space occupied by the honey in the larger jar is 50% * 11 jars = 55% of the smaller jar. So, the space occupied by the honey in the smaller jar is 25% since the larger jar takes up 55%. \n",
    "In order to calculate the number of jellybeans in the jars, we need to convert the space occupied to liters. From there, it is a simple matter of converting the honey to jellybeans using the conversionThe Assistant evaluate the problem and the solution. The assistant first evaluates and find out where the solution is wrong. Then the assistant provides the verification. The evaluation process and verification are enclosed within <evaluate></evaluate> and <verify></verify> tags, respectively, i.e., <evaluate>evaluation process here</evaluate> <verify>verification here</verify>. Verification is limited to 'correct' or 'wrong'. Assistant:<evaluate>Based on the scenario described, we first need to convert 2.5 hours movie duration to minutes, which is 2.5 * 60 = 150 minutes. Then, we need to calculate the number of times John uses the bathroom during the movie by dividing the total movie duration (150 minutes) by the interval (50 minutes per bathroom). This gives us an answer of 150 minutes / 50 minutes per bathroom = 3 bathroom usages.\n",
    "</evaluate> <verify>correct</verify>\n",
    "Assistant: The user asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively.'''\n",
    "\n",
    "tags = extract_tags_eval(\"<think>\" + text.replace(\"<<\", \"\").replace(\">>\", \"\"))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110.0, 0.0, 1, 1.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped_correctness_reward_eval(\"15\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wrong'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verification = tags[\"verify\"][-1].lower() if len(tags[\"verify\"]) > 2 else \"\"\n",
    "verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if verification == \"correct\" or verification == \"wrong\":\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 0.0, 0.0, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_trajectory.query_responses A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think>reasoning process here</think> <answer>answer here</answer>. User: Herbert is 10 years younger than Kris. If Kris is 24 years old now, how old will Herbert be next year?. Assistant: <think>First, let's find Herbert's current age by subtracting 10 years from Kris' current age. Herbert is 24 - 10 = 14 years old. Next, we want to know how old Herbert will be next year. We take Herbert's current age and add one year, so Herbert will be 14 + 1 = 15 years old next year.</think> <answer>15</answer>The Assistant evaluate the problem and the solution. The assistant first evaluates and find out where the solution is wrong. Then the assistant provides the verification. The evaluation process and verification are enclosed within <evaluate></evaluate> and <verify></verify> tags, respectively, i.e., <evaluate>evaluation process here</evaluate> <verify>verification here</verify>. Verification is limited to 'correct' or 'wrong'. Assistant:<evaluate>According to reasoning, Herbert will be 15 years old next year. The assistant thinks that the instructions state correctly that Herbert will be 15 years old next year.</evaluate> <verify>wrong</verify><|endoftext|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = extract_tags_eval(\"<think>\" + text.replace(\"<<\", \"\").replace(\">>\", \"\"))\n",
    "result[\"answer\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(2, 0), dtype=torch.bool)\n",
      "tensor([], size=(2, 0))\n"
     ]
    }
   ],
   "source": [
    "from torchtune import config, generation, modules, rlhf, training, utils\n",
    "import torch\n",
    "responses = torch.tensor([[],[] ])\n",
    "(\n",
    "        response_padding_masks,\n",
    "        responses,\n",
    "    ) = rlhf.truncate_sequence_at_first_stop_token(  # [B x G, L]\n",
    "        responses, torch.tensor(128012), torch.tensor(128011)\n",
    "    )\n",
    "print(response_padding_masks)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "argmax(): Expected reduction dim 1 to have non-zero size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#[rank1]: IndexError: argmax(): Expected reduction dim 1 to have non-zero size.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m seq_lens \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_unmasked_sequence_lengths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_padding_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(seq_lens)\n",
      "File \u001b[0;32m~/Github/llm_marl/torchtune/torchtune/training/pooling.py:40\u001b[0m, in \u001b[0;36mget_unmasked_sequence_lengths\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mReturns the sequence lengths (0-indexed) for each batch element, excluding masked tokens.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# calculate per-batch-element sequence lengths by finding last valid tokens\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m sequence_lengths \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequence_lengths\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m0\u001b[39m, mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: argmax(): Expected reduction dim 1 to have non-zero size."
     ]
    }
   ],
   "source": [
    "#[rank1]: IndexError: argmax(): Expected reduction dim 1 to have non-zero size.\n",
    "seq_lens = training.get_unmasked_sequence_lengths(response_padding_masks)\n",
    "print(seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(1, 0), dtype=torch.int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.tensor([[128012, 128011]]).shape[1]\n",
    "\n",
    "torch.tensor([[128012, 128011]])[:,l:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "answer = '$69'\n",
    "if '69' == answer:\n",
    "    print(\"yes\")\n",
    "elif '69' in answer:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
